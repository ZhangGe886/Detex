{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "This segment of the tutorial will teach you how to preform waveform similarity clustering in detex. The function used to preform clustering is createCluster of the construct module. The results are then stored in an instance of the ClusterStream class. Let's start by looking at the createCluster function and some important parameters that go into it, then we will look at the ClusterStream class its methods.\n",
    "\n",
    "## CreateCluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import detex\n",
    "version = detex.__version__\n",
    "print (\"Detex version is %s\\n\" % version)\n",
    "print (detex.construct.createCluster.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there are a lot of input arguments, and a lot to think about when creating a cluster object. Let me elaborate on some of the arguments you should pay special attention to. \n",
    "\n",
    "* fet_arg - make sure to look at the detex.getdata.quickFetch docs for this one. Basically, if you want to use a custom DataFetcher be sure to pass it to the createCluster call here or else detex will try to use a local directory with the default name of EventWaveForms.\n",
    "\n",
    "* filt - parameters to apply a bandpass filter to the waveform similarity clustering and ALL all detex downstream operations. Make sure to think about this carefully before simply using the default, as the default values are not appropriate for all data sets.\n",
    "\n",
    "* fillZeros - a parameter for handling data with gaps. If data are not avaliable for the entire range (defined by template key and trim parameter) detex will simply fill zeros so that each trace will have the length defined by the trim parameter. The created cluster instance can then be used later on by detex, although you should be careful going forward to no include a bunch of the zero data in your detector, more on that later. \n",
    "\n",
    "* trim - a two element list that defines the length of each waveform. The first element is the time before the origin (as reported in the station key) and the second element is the number of seconds after the reported origin time. \n",
    "\n",
    "### Dealing with gaps\n",
    "\n",
    "In order to see how some of these parameters affect the clustering process we will look at an early UUSS dataset that has some issues with gaps. Here are the stations and templates:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stakey = detex.util.readKey('StationKey.csv', key_type='station')\n",
    "stakey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temkey = detex.util.readKey('TemplateKey.csv', key_type='template')\n",
    "temkey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because there are so many events it may take some time to get the data. We will skip getting the continuous data because it is not needed for this section of the tutorial. We should probably also start the logger in case we need more info than what is printed to the screen. We will also delete an old logger if there is one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.exists(\"detex_log.log\"):\n",
    "    os.remove(\"detex_log.log\")\n",
    "detex.setLogger()\n",
    "detex.getdata.makeDataDirectories(getContinuous=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will cluster these events while varying the input arguments. Let's start by using the defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time cl = detex.createCluster() # notice we can call createCluster from the detex level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the wall time for the createCluster call was around 2 minutes (on my computer). Let's make a function to see how many of the original 220 events were actually used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cluster(cl):\n",
    "    for c in cl:\n",
    "        sta = c.station\n",
    "        num_events = len(c.key)\n",
    "        print '%s had %d events used in the analysis' % (sta, num_events)\n",
    "    print '\\n'\n",
    "def get_unused_events(cl, temkey):\n",
    "    for c in cl:\n",
    "        sta = c.station\n",
    "        unused = list(set(temkey.NAME) - set(c.key))\n",
    "        print 'Unused events on %s are:\\n %s\\n' % (sta, unused)\n",
    "\n",
    "def get_info(cl, temkey_in='TemplateKey.csv'):\n",
    "    temkey = detex.util.readKey(temkey_in, 'template')\n",
    "    print 'There are %d events in the template key' % len(temkey) \n",
    "    check_cluster(cl)\n",
    "    get_unused_events(cl, temkey)\n",
    "\n",
    "get_info(cl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now let's try using fillZeros as True rather than the default of False. This will force each event waveform to be exactly the length defined by the trim parameter by filling with zeros where necessary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time cl2 = detex.createCluster(fillZeros=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_info(cl2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So setting fill_zeros to True caused detex to use all the events on MSU and all but four on IMU. The four IMU events that went unused were probably due to missing waveforms. We can verify this by looking in the log for indications the that the data were not available to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = detex.util.readLog()\n",
    "log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Time Trials\n",
    "If you are trying to perform waveform clustering on a large data set it may be worth your time to understand how varying certain parameters can affect runtimes. Let's isolate a few variables and compare run times from the default values. If you are running this on your computer at home it may take some time, skip ahead if you aren't interested. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup code for time trials\n",
    "import time\n",
    "def timeit(func): # decorator for timing function calls\n",
    "    def wraper(*args, **kwargs):\n",
    "        t = time.time()\n",
    "        out = func(*args, **kwargs)\n",
    "        return (time.time() - t, out)\n",
    "    return wraper\n",
    "\n",
    "@timeit\n",
    "def time_cluster(*args, **kwargs):\n",
    "    detex.createCluster(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "detex.verbose = False # silence detex\n",
    "\n",
    "cols = ['waveform_duration', 'run_time']\n",
    "df = pd.DataFrame(columns=cols)\n",
    "\n",
    "trims = [(10, 120), (5, 60), (2, 30), (1, 15)]\n",
    "\n",
    "for trim in trims:\n",
    "    rt = time_cluster(trim=trim)[0]\n",
    "    ser = pd.Series([sum(trim), rt], index=cols)\n",
    "    df.loc[len(df)] = ser\n",
    "    \n",
    "plt.plot(df.waveform_duration, df.run_time)\n",
    "plt.title(\"Waveform Length vs Run Times\")\n",
    "plt.ylabel(\"run times (seconds)\")\n",
    "plt.xlabel(\"waveform lengths (seconds)\")\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['num_events', 'run_time']\n",
    "df = pd.DataFrame(columns=cols)\n",
    "\n",
    "temkey = detex.util.readKey(\"TemplateKey.csv\", \"template\")\n",
    "\n",
    "temkey_lengths = [10, 20, 50, 100, 150, 200]\n",
    "\n",
    "for tkl in temkey_lengths:\n",
    "    temkey2 = temkey.copy()\n",
    "    \n",
    "    rt = time_cluster(templateKey=temkey2[:tkl+1])[0]\n",
    "    ser = pd.Series([tkl, rt], index=cols)\n",
    "    df.loc[len(df)] = ser\n",
    "    \n",
    "plt.plot(df.num_events, df.run_time)\n",
    "plt.title(\"Number of Events vs Runtimes\")\n",
    "plt.xlabel(\"Number of Events\")\n",
    "plt.ylabel(\"Runtimes (seconds)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Although a bit more complicated than this, we could qualitatively estimate that changing the waveform length scales the runtime by approximately N (linearly with time) whereas the number of events scales the runtime by approximately N<sup>2</sup> (quadratic with time). Let's see how decimating the data changes the runtimes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test various decimation factors\n",
    "rt_base = time_cluster()[0]\n",
    "rt_decimate = time_cluster(decimate=2)[0]\n",
    "print(\"Base run time: %.02f, Decimated run time: %.02f\" % (rt_base, rt_decimate))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, this didn't seem to make much of a difference. The original data were sampled at 100 Hz so using a decimation factor of 2 would have reduced the sampling rate to 50 Hz. Since we left the default bandpass filter (1.0 to 10.0 Hz) it might make sense to use a decimation factor of 4 in order to bring the sampling rate down to 25 Hz. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## ClusterStream and Cluster Classes\n",
    "\n",
    "The ClusterStream and Cluster classes are used to control and visualize waveform similarity clustering. These classes are required to define the subspaces used in the detection process.\n",
    "\n",
    "The ClusterStream is a container for one or more Cluster instances. There is a cluster instance for each station, although most attributes are accessible from the ClusterStream level. Let's take create a ClusterStream instance and take a closer look.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import detex # reimport so we can start here\n",
    "detex.verbose = False\n",
    "cl = detex.createCluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bulk of the information for the ClusterStream is stored in the trdf attribute, which, of course, is a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.trdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this DataFrame there is a row for each station. The columns are:\n",
    "\n",
    "| Column | Description |\n",
    "|:-----:| :---------: |\n",
    "| CCs | A matrix of max correlation coef for each station pair |\n",
    "| Lags | A matrix of lag samples corresponding to the highest correlation coef |\n",
    "| Subsamp | The decimal fraction determined by subsample extrapolation |\n",
    "| Events | The name of the events used |\n",
    "| Stats | Selected stats of the events |\n",
    "\n",
    "The CCs and Lags are DataFrames that have indices and rows that correspond to an element in the Events list. This is probably best illustrated by an example. Let's say we want to find the max correlation ceof. between two events and the corresponding number of samples that would be required to shift the first event to line up with the second. First, we need to find where the events we want to find occur in the events list, then we can index them in the lags and ccs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are two events in the list\n",
    "ev1 = '2010-07-10T08-57-51.25'\n",
    "ev2 = '2014-11-29T14-18-04.87'\n",
    "events = list(cl.trdf.loc[0, 'Events']) # cast from np array to list\n",
    "# Find the index where each event occurs in the list\n",
    "ev1_ind = events.index(ev1)\n",
    "ev2_ind = events.index(ev2)\n",
    "print (\"%s index is %d, %s index is %d\" % (ev1, ev1_ind, ev2, ev2_ind))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = cl.trdf.loc[0, 'CCs']\n",
    "lags = cl.trdf.loc[0, 'Lags']\n",
    "coef = cc.loc[ev1_ind, ev2_ind]\n",
    "lag = lags.loc[ev1_ind, ev2_ind]\n",
    "print (coef, lag)\n",
    "# events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Visualization Methods\n",
    "The ClusterStream has several methods for visualizing. We can create a simple similarity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.simMatrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the events (x and y axis) are ordered based on origin time. We can also plot them based on the groups the events best fit in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.simMatrix(groupClusts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize and change the clustering structure for each station with the dendro and updateReqCC methods, just as in the intro tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.dendro()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl[0].updateReqCC(.6)\n",
    "cl[0].dendro()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the spatial relations of the events with the plotEvents method. This is used to get a quick and dirty idea of event locations and depths; it still needs a lot of work before it will produce presentable plots. The following is not the best example of a meaningful plot because there are so many colors and different groups but plotEvents can be useful, especially on smaller datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl[0].plotEvents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next section\n",
    "The [next section](../SubspaceDetection/subspace_detection1.md) covers subspace detection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
